# -*- coding: utf-8 -*-
"""Parkinsons.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lwe6Dq5cFUSFVXzI80QXt9PA30eoyGit
"""

#importing libraries

import pandas as pd
import numpy as np
import os,sys
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import seaborn as sns

#read the data

Park_data = pd.read_csv('parkinsons.data')
Park_data.head()

#get features and labels

features = Park_data.loc[:,Park_data.columns!='status'].values[:,1:]
labels = Park_data.loc[:,'status'].values

#get the count of each label (0 and 1 ) in labels

print(labels[labels==1].shape[0], labels[labels==0].shape[0])

#visualising the data


sns.pairplot(Park_data, x_vars='status',y_vars=['MDVP:Fo(Hz)','MDVP:Fhi(Hz)','MDVP:Flo(Hz)','NHR','HNR'])

# scale features betweeen -1 and 1

scaler=MinMaxScaler((-1,1))
x=scaler.fit_transform(features)
y=labels

#split into train and test set

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state=1)

#train the model

model=XGBClassifier()
model.fit(x_train,y_train)

#calculate the accuracy

y_pred = model.predict(x_test)
print("accuracy = ", accuracy_score(y_test,y_pred)*100)

#making the confusion matrix

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot = True)

#classification report

print(classification_report(y_test, y_pred))

